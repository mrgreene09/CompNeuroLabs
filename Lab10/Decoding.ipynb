{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Neural Decoding</span>\n",
    "\n",
    "\n",
    "In lab, we will be working with files in the Zhang_neurons folder. This dataset contains recordings from 132 neurons in a monkey's inferior temporal lobe (IT). an area known to be highly involved in high-level vision and object perception. The recordings were made while a monkey viewed 7 different objects that were presented at each of three screen locations. Each object was presented approximately 20 times at each of the three locations. In each trial, the monkey viewed a fixation dot for 500 ms, and then viewed one of the seven objects for another 500 ms. The data were reported in Zhang et al (2011, *PNAS*). \n",
    "\n",
    "Note: This paper contains conditions in which objects were presented simultaneously, but only the single object condition is included.\n",
    "\n",
    "Zhang, Y., Meyers, E. M., Bichot, N. P., Serre, T., Poggio, T. A., & Desimone, R. (2011). *Object decoding with attention in inferior temporal cortex*. Proceedings of the National Academy of Sciences, 108(21), 8850-8855.\n",
    "\n",
    "https://doi.org/10.1073/pnas.1100999108\n",
    "\n",
    "### About the dataset:\n",
    "\n",
    "The data are in raster format, meaning that each .mat file contains data from one of the 132 neurons. Each of these files contains three variables.\n",
    "\n",
    "*raster_site_info*: A structure corresponding to the recording parameters of the experiment that <u>can be ignored</u> for the purpose of this problem set.\n",
    "\n",
    "*raster_labels*: A structure that contains the object being viewed (stimulus_ID), the position of the object (stimulus_position), and the combined object+position (combined_ID_position).\n",
    "\n",
    "*raster_data*: A matrix where each row corresponds to the data from one trial, and each column corresponds to data from one 1-ms time point (the rows are also in order so that the first trial is in the first row, and the last trial is in the last row).\n",
    "\n",
    "### Working with the dataset:\n",
    "\n",
    "Dealing with 132 separate data files can be a challenge. First, import these packages and define some helpful code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mat2array import loadmat\n",
    "import glob\n",
    "import os\n",
    "\n",
    "homeDirectory = os.getcwd()\n",
    "os.chdir(homeDirectory+ '/Zhang_neurons')\n",
    "neuronList = glob.glob('*.mat')\n",
    "os.chdir(homeDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I recommend reading the files in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neuronList)):  \n",
    "    path = homeDirectory + '/Zhang_neurons/' + neuronList[i]\n",
    "    \n",
    "    neuron = loadmat(path)\n",
    "    \n",
    "    raster_data = neuron['raster_data']\n",
    "    stimID = neuron['raster_labels']['stimulus_ID']\n",
    "    stimPosition = neuron['raster_labels']['stimulus_position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will only concern ourselves with the seven object identities. It's helpful to define them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['car','couch','face','flower','guitar','hand','kiwi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the indices of the first class (car), you could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carInd, = np.where(stimID == classes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all IT neurons are equally responsive to visual stimuli. Calculate the mean spike count rate for each neuron in the interval from 601-1000 ms and plot a histogram of the population's spiking rate. (We're omitting the first 100 ms because there is little visually-driven activity in this area during this period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data storage for the average firing rate\n",
    "meanRate = np.zeros()\n",
    "# Loop through each neuron in neuronList\n",
    "for i in range():  \n",
    "    # Define the file path for loading the .mat files\n",
    "    \n",
    "    # Use the loadmat function to load the file\n",
    "    \n",
    "    # Defining the data stored in the .mat file\n",
    "    raster_data = neuron['raster_data']\n",
    "    stimID = neuron['raster_labels']['stimulus_ID']\n",
    "    stimPosition = neuron['raster_labels']['stimulus_position']\n",
    "    \n",
    "    # Calculate the mean spike count rate for each neuron and store in meanRate\n",
    "\n",
    "# Plotting\n",
    "plt.hist(meanRate);\n",
    "plt.title('Histogram of population firing rates')\n",
    "plt.xlabel('Firing rate')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2-3 sentences: \n",
    "\n",
    "What do you conclude about the visual responsiveness to this population? What might be a negative consequence of decoding using these raw firing rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to turn the raw raster plots into spike-count rate matrices. We have provided you with a function that computes the firing rate matrix for a neuron and creates a 420-trial by 18-time bin matrix in which each time bin represents the spike count rate for a neuron within the time window. The time bins begin every 50 ms (1 ms, 51 ms, 101 ms, etc), and are 150 ms long. Thus, time window 1 is from 1-150, window 2 is from 51-200, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = loadmat('Zhang_neurons/bp1001spk_01A_raster_data.mat')\n",
    "def rate(neuron):\n",
    "    global bins\n",
    "    bins = np.arange(0,890,50)\n",
    "    rateMat = np.zeros((420,18))\n",
    "    raster_data = neuron['raster_data']\n",
    "    for i in range(len(bins)):\n",
    "        rate1 = raster_data[:,bins[i]:bins[i]+150]\n",
    "        rate2 = np.sum(rate1,axis=1)\n",
    "        rate3 = rate2/.15\n",
    "        rateMat[:,i] = rate3\n",
    "    \n",
    "    return rateMat\n",
    "\n",
    "# Example using the new function\n",
    "rateMat = rate(neuron)\n",
    "print(rateMat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix the problems you outlined above, you want to z-score the firing rates for this neuron. Recall that a z-score is calculated as follows:$$z = \\frac{x-\\mu}{\\sigma}$$ Where $x$ is the raw firing rate, $\\mu$ is the mean firing rate, and $\\sigma$ is the standard deviation of the cell's firing rate.\n",
    "\n",
    "Use the zScore function to find the z-score of your firing rate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScore(rateMat): \n",
    "    globalMean = np.mean(rateMat)\n",
    "    globalSTD = np.std(rateMat)\n",
    "    z = (rateMat - globalMean)/globalSTD\n",
    "    return z\n",
    "\n",
    "# Apply the zScore function to your firing rate matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to do some decoding!\n",
    "\n",
    "In the first problem, we will examine how much each neuron knows about each of the seven object categories. Fill in the template below in order to do the decoding. Recall that you used a correlation classifier last week on one neuron and found a classification accuracy of ~19%.\n",
    "\n",
    "NOTE: Because some neurons are missing one trial, we will skip over them for now.\n",
    "\n",
    "NOTE: To have you work through 10-fold cross validation in a manageable way, I'm having you use random indices. This may result in slightly biased numbers in the training-testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the support vector machine classifier\n",
    "from sklearn import svm\n",
    "classify = svm.SVC(kernel='linear')\n",
    "\n",
    "#Initialize data structures\n",
    "trialInds = np.zeros()\n",
    "totalAccuracy = np.zeros(125) # the number of neurons with all trials\n",
    "\n",
    "# Define classes\n",
    "classes = ['car','couch','face','flower','guitar','hand','kiwi']\n",
    "\n",
    "# Start a cell count at 0\n",
    "count = 0\n",
    "\n",
    "for i in range():\n",
    "    # Define the file path for loading the .mat files\n",
    "    \n",
    "    # Use the loadmat function to load the file\n",
    "    \n",
    "    # Defining the data stored in the .mat file\n",
    "    raster_data = neuron['raster_data']\n",
    "    stimID = neuron['raster_labels']['stimulus_ID']\n",
    "    stimPosition = neuron['raster_labels']['stimulus_position']\n",
    "    \n",
    "    # Conditional to check if neuron has all 420 trials\n",
    "    if raster_data.shape[0] == :\n",
    "        \n",
    "        # Calculate rate and zScores of the neuron\n",
    "        rateMat = rate()\n",
    "        z = zScore()\n",
    "        \n",
    "        # Loop to define the trial indices\n",
    "        for k in range():\n",
    "            classInds, =  np.where()\n",
    "            trialInds[classInds] = \n",
    "        \n",
    "        # Creating random indices for 10-fold cross validation\n",
    "        inds = np.random.randint(0, high=10, size=(420))\n",
    "        \n",
    "        # 10-fold cross validation\n",
    "        for j in range(10):\n",
    "            # define testing data\n",
    "            testInds =\n",
    "            testVec = trialInds[]\n",
    "            testData = z[]\n",
    "\n",
    "            trainInds = \n",
    "            trainVec = trialInds[]\n",
    "            trainData = z[]\n",
    "            \n",
    "            # Train SVM\n",
    "            classify.fit(trainData,trainVec)\n",
    "            \n",
    "            # Run SVM on testing data to get predictions of image class\n",
    "            predClass = classify.predict()\n",
    "            \n",
    "            # Initialize storage space to calculate accuracy\n",
    "            accuracy = np.zeros()\n",
    "            \n",
    "            # Loop through predClass\n",
    "            for h in range():\n",
    "                # Conditional to check accuracy of predClass with respect to testVec\n",
    "        \n",
    "        # Calculate and store accuracy\n",
    "        totalAccuracy[count] = \n",
    "        count = count + 1\n",
    "        \n",
    "# Calculate average accuracy across all neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the classifier was randomly guessing object categories, how well would it do? How well do these cells do relative to that standard? Is it statistically significant?\n",
    "\n",
    "Hint: From **_scipy.test.mstats_** import the ***ttest_onesamp()*** function to quantify your results. Is it practically significant? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import ttest_onesamp\n",
    "ttest_onesamp()\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it's time to use the entire population to decode. \n",
    "First, calculate the z-scores for each of the neurons as you did in the first decoding problem, except this time you will need to store them into a 3-dimensional matrix. The purpose of this is to decode by the entire population of neurons at each time point, rather than an individual neuron. \n",
    "\n",
    "You will need two total loops: The first is to calculate the z-scored matrix for all of the neurons, the second will be used to do the decoding. NOTE: because objects were presented to each neuron in a random order, we will order each neuron's data by object class before decoding.\n",
    "\n",
    "Calculate and plot the classifier's accuracy of the population of neurons per time point.\n",
    "\n",
    "<img src=\"image2.jpg\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: create 3D matrix\n",
    "\n",
    "# Import relevant machine learning tools\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import svm\n",
    "classify = svm.SVC(kernel='linear')\n",
    "\n",
    "#Initialize data structures\n",
    "trialInds = np.zeros()\n",
    "nMat = np.zeros()\n",
    "\n",
    "# Define classes\n",
    "classes = ['car','couch','face','flower','guitar','hand','kiwi']\n",
    "\n",
    "# Start a cell count at 0\n",
    "count1 = 0\n",
    "for i in range():\n",
    "    # Define the file path for loading the .mat files\n",
    "    \n",
    "    # Use the loadmat function to load the file\n",
    "    \n",
    "    # Defining the data stored in the .mat file\n",
    "    raster_data = neuron['raster_data']\n",
    "    stimID = neuron['raster_labels']['stimulus_ID']\n",
    "    stimPosition = neuron['raster_labels']['stimulus_position']\n",
    "\n",
    "    \n",
    "    # Conditional to check if neuron is missing a trial\n",
    "    if raster_data.shape[0] == :\n",
    "        # Calculate rate and zScores of the neuron\n",
    "        rateMat = rate()\n",
    "        z = zScore()\n",
    "        \n",
    "        # Loop to define the trial indices\n",
    "        for k in range():\n",
    "            classInds, =  np.where()\n",
    "            trialInds[classInds] = \n",
    "        \n",
    "        # Sorting the data to be used later\n",
    "        sortedTrials = sorted(enumerate(trialInds), key=lambda x:x[1])\n",
    "        sortedTrials = np.asarray(sortedTrials)\n",
    "        sortedData = sortedTrials[:,0].astype(int)\n",
    "        img = sortedTrials[:,1].astype(int)\n",
    "        \n",
    "        nMat[:,:,count1] = z[sortedData,:]\n",
    "        count1 = count1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: decoding the population\n",
    "\n",
    "# Initializing storage space\n",
    "totalAccuracy = np.zeros()\n",
    "\n",
    "# Loop through each 18 time bins\n",
    "for t in range():\n",
    "    \n",
    "    # Define 420x125 feature vector per time bin\n",
    "    timeMat = nMat[]\n",
    "    \n",
    "    # Define random indices for 10-fold cross validation\n",
    "    randInds = np.random.randint(0, high=10, size=(420))\n",
    "    \n",
    "    # 10-fold cross validation\n",
    "    for j in range():\n",
    "        # define testing data\n",
    "        testInds = \n",
    "        testVec = img[]\n",
    "        testData = timeMat[]\n",
    "\n",
    "        trainInds = \n",
    "        trainVec = img[]\n",
    "        trainData = timeMat[]\n",
    "        \n",
    "        # Train SVM\n",
    "        classify.fit(trainData,trainVec)\n",
    "        \n",
    "        # Run SVM on testing data\n",
    "        predClass = classify.predict()\n",
    "        \n",
    "        # Initialize data storage to calculate accuracy\n",
    "        accuracy = np.zeros()\n",
    "        \n",
    "        # Loop through predClass\n",
    "        for h in range():\n",
    "            # Conditional to check accuracy of predClass with respect to testVec\n",
    "    \n",
    "    # Calculate accuracy for each time bin\n",
    "    totalAccuracy[t] = \n",
    "\n",
    "# Plotting accuracy of population with respect to each time bin\n",
    "plt.figure()\n",
    "plt.plot(bins,totalAccuracy)\n",
    "plt.xlabel(\"FILL ME IN\")\n",
    "plt.ylabel(\"FILL ME IN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went according to plan, your decoding graph should look qualitatively similar to the blue curve from the original paper:\n",
    "\n",
    "<img src=\"image1.png\" alt=\"drawing\" width=\"250\"/>\n",
    "\n",
    "Note: Zhang et al used a different type of classifier, so your accuracies will be subtely differnet.\n",
    "\n",
    "Compare what you found here to what you found when you looked at each cell individually? Is information about object identity primarily found in individual cells or across the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
