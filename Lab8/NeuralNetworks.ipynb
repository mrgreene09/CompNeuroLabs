{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Neural Networks</span>\n",
    "\n",
    "### Part 1: Single McCulloch-Pitts Neuron\n",
    "\n",
    "Fill in the following script to complete the perceptron. Your code should be able to do the following:\n",
    "\n",
    "- Define initial weights randomly by sampling from a uniform distribution between -1 and +1. Assign a constant bias of -1, and a learning rate of $\\eta$ = 0.01.\n",
    "- Train your MCP with 500 iterations.\n",
    "- Within each iteration of training, compute the neuron's activation (output) as the weighted sum of the input data, weights, and bias. This should be done in one line of code considering all training examples from your input data at once. Hint: apply *matmul* function\n",
    "- Apply the sign activation function to the weighted sum.\n",
    "- Map the output of the activation function to binary categories according to the following rule: outputs < 0 will be assigned to category 0 while outputs > 0 will be assigned to category 1. These are the guesses of the perceptron.\n",
    "- Compute the error between the guesses and the expected output.\n",
    "- Adjust the weights based on the error\n",
    "- Compute the percent accuracy of the response at each iteration. Save this value for plotting\n",
    "- With the truth table below serving as input data (i.e. A and B) and Y serving as the expected output, train your MCP neuron to learn the logical AND operation. Plot the accuracy of the neuron as a function of iteration\n",
    "\n",
    "|  A  |  B  |  C  |\n",
    "|-----|-----|-----|\n",
    "|  1  |  1  |  1  |\n",
    "|  1  |  0  |  0  |\n",
    "|  0  |  1  |  0  |\n",
    "|  0  |  0  |  0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize weights, bias, input, output, and eta\n",
    "\n",
    "# Initialize data structure to hold accuracy\n",
    "accuracy = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Calculate output\n",
    "\n",
    "    # Update yHat \n",
    "\n",
    "    # Calculate error\n",
    "\n",
    "    # Compute delta weights\n",
    "\n",
    "    # Update weights\n",
    "    \n",
    "    # Store accuracy\n",
    "    \n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(accuracy)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('percent accuracy')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alter the expected outputs to reflect the exlusive or (XOR) problem. Submit a plot of accuracy as a function of iteration. Tip: Do NOT use a while loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wieghts, bias, input, output, and eta\n",
    "\n",
    "# Initialize data structure to hold accuracy\n",
    "accuracy = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Calculate output\n",
    "\n",
    "    # Update yHat\n",
    "\n",
    "    # Calculate error\n",
    "\n",
    "    # Compute delta weights\n",
    "\n",
    "    # Update weights\n",
    "    \n",
    "    # Store accuracy\n",
    "    \n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(accuracy)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('percent accuracy')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what it means for XOR to not be a linearly separable problem. Feel free to include any supplementary plots that bolster your argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the smallIrisData.mat into your notebook with the **_scipy.io loadmat_** function. Write an inner loop that trains your neural network with 200 iterations and an outer loop that trains your neuron 10 times. Store the 100 final weights after each iteration.\n",
    "\n",
    "Recall that the classification boundary for this problem is: \n",
    "\n",
    "$$w^Tx+b=0$$ \n",
    "\n",
    "Create a scatterplot of the input data, with class 0 flowers shown in filled blue circles and class 1 flowers shown in filled red circles. Plot lines for each of the 10 decision boundaries given by the final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages to load data\n",
    "import scipy.io as sio\n",
    "# Run this:\n",
    "smallIrisData = sio.loadmat('smallIrisData.mat')\n",
    "data = smallIrisData['data']\n",
    "y = smallIrisData['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize eta\n",
    "\n",
    "# Initialize data storage for final weights\n",
    "final_w = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Initialize weights, bias, yHat, and storage for accuracy\n",
    "\n",
    "    for j in range():\n",
    "        # Calculate output\n",
    "\n",
    "        # Update yHat\n",
    "\n",
    "        # Calculate error\n",
    "\n",
    "        # Compute delta weights\n",
    "\n",
    "        # Update weights\n",
    "\n",
    "    # Store final weights in vector\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(data[:50,0], data[:50,1])\n",
    "plt.scatter(data[50:,0], data[50:,1])\n",
    "\n",
    "xAxis = np.linspace(0,7,num=100)\n",
    "for k in range(10):\n",
    "    w_line = -(final_w[k,0])/(final_w[k,1])*xAxis - (bias/final_w[k,1])\n",
    "    plt.plot(xAxis,w_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude about the possible ways that this problem can be solved? In other words, why are the solutions not unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the MCP neuron generalize? Break up the smallIrisData into a set of 80 training points (40 from each class), along with the corresponding vector *y* as training data. Save the remaining 20 points in a testingSet and testingVector respectively. Train your perceptron with 500 iterations of the 80 training points. Afterwards, test the neuron with the 20 testing points. Hint: use your final weights to do a forward pass, and do not adjust the weights. How well does the model generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and eta\n",
    "\n",
    "# Pre-allocate space\n",
    "newIrisData = np.zeros()\n",
    "new_y = np.zeros()\n",
    "testData = np.zeros()\n",
    "test_y = np.zeros()\n",
    "\n",
    "# Split data into training and testing trials\n",
    "newIrisData[:40,:] = \n",
    "newIrisData[39:,:] = \n",
    "new_y[:39,:] = \n",
    "new_y[39:,:] = \n",
    "\n",
    "testData[:9,:] = \n",
    "testData[9:,:] = \n",
    "test_y[:9,:] = \n",
    "test_y[9:,:] = \n",
    "\n",
    "for i in range():\n",
    "\n",
    "        # Calculate output\n",
    "\n",
    "        # Update yHat\n",
    "\n",
    "        # Calculate error\n",
    "\n",
    "        # Calculate delta weights\n",
    "\n",
    "        # Update weights\n",
    "        \n",
    "# Calculate accuracy of MCP on the test data\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Sigmoid Networks\n",
    "\n",
    "Consider the letters \"X\" and \"O\". These can be represented by 5x5 matrices in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,0,0,0,1],[0,1,0,1,0],[0,0,1,0,0],[0,1,0,1,0],[1,0,0,0,1]])\n",
    "O = np.array([[0,1,1,1,0],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[0,1,1,1,0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be making a single-layer neural network with two sigmoid neurons to learn what defines and X and an O.\n",
    "\n",
    "Fill in the following script to complete your neural network. Your code should be able to do the following:\n",
    "- Implement the sigmoid activation function. This will take in the raw weighted input z and output the following: $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- Write a line of code that returns the derivative of the sigmoid function evaluated at z. Hint: Here is the equation:$$\\sigma'(z)=\\sigma(z)*(1-\\sigma(z))$$\n",
    "- Define your input data. This will be a 25x2 matrix containin the values of X in the first column and the values of O in the second column.\n",
    "- Define your desired output. You will want to have two sigmoid units, one tuned to X and the other to O. The desired output for X is [1,0] and the desired output for O is [0,1]. These will be placed into one 2x2 matrix.\n",
    "- Initialize your weight matrix and bias vector. Although we would normally initialize them with random values, please initialize all weights to 0.5 and biases to -0.5 for replicability.\n",
    "- In an iterative loop, compute a and z. You may choose to have an innermost loop iterating between X and O or (preferred) do this in one matrix operation.\n",
    "- Compute the error of your output with the following equation: $$\\delta=(a-y)\\odot\\sigma'(z)$$ Recall that y will be the desired output, that you will be using your sigmaPrime code and to use the elementwise multiplication for $\\odot$.\n",
    "- Update your weights and biases according to the following equation: $$w_{i+1}=w_{i}-\\eta*x^T$$ $$b_{i+1}=b_{i}-\\eta*\\delta$$\n",
    "- Compute the total quadratic cost of each iteration: $$C = \\frac{1}{2n}\\Sigma_{x}(y(x)-a^L(x))^2$$\n",
    "Using an $\\eta$ of 0.01, run your network for 10,000 iterations. Plot your cost of these iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input\n",
    "inputX = X.flatten(order='F')\n",
    "inputO = O.flatten(order='F')\n",
    "data = np.zeros((25,2))\n",
    "data[:,0] = inputX\n",
    "data[:,1] = inputO\n",
    "\n",
    "# Define output\n",
    "\n",
    "# Initialize weights, number of iterations, eta, and bias\n",
    "\n",
    "# Pre-allocate space for cost and mean cost vector\n",
    "costVec = np.zeros()\n",
    "meanCost = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    \n",
    "    # Calculate raw activation z\n",
    "    \n",
    "    # Calculate activation after sigmoid function\n",
    "    \n",
    "    # Calculate error\n",
    "    \n",
    "    # Update weights and bias\n",
    "    \n",
    "    # Calculate and store cost in a vector\n",
    "\n",
    "# Calculate the mean cost\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(np.arange(n),meanCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you see. Why does the cost pattern look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the network learn? Reshape the learned wieght matrix into two 5x5 matrices and visualize them with the **_plt.imshow_** function. \n",
    "\n",
    "Hint: the **_np.reshape()_** function will be very helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectors\n",
    "Xweight = weight[:,0]\n",
    "Oweight = weight[:,1]\n",
    "\n",
    "# Reshaping them into images\n",
    "Xweight = np.reshape(##,##, order='F')\n",
    "Oweight = np.reshape(##, ##, order='F')\n",
    "\n",
    "# Flipping the sign to maintain contrast\n",
    "Xweight[Xweight<.52] = Xweight[Xweight<.52]*-1\n",
    "Oweight[Oweight==.5] = Oweight[Oweight==.5]*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Xweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Oweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe how the pattern of weights relate to the X and O patterns, and why this might be helpful to recognizing these letters. **At this point, save your weights and biases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, we have to recognize letters in different fonts and in different handwriting. How well does your neural network do with slightly different X's and O's? Add 10% noise to your X and O images by randomly flipping 10% of pixels from 1 to 0 and vice versa. Using your previously learned weights, do a forward pass through your network to see how well the network can classify the altered digits.\n",
    "\n",
    "Hint: Use **_np.where()_** to find the indices where your random numbers cross some threshold. Those indices can be used to index your input data and flip 10% of the pixels. Keep in mind that the **_np.where()_** function returns values in a tuple, so to access the indices, follow your variable with a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two 25x1 matrices of random numbers (uniformly distributed btwn 0 and 1)\n",
    "\n",
    "# Find indices where random numbers cross threshold\n",
    "\n",
    "# For-loop to flip pixels on X image\n",
    "for i in range():\n",
    "    # Conditionals to flip pixels\n",
    "        \n",
    "# For-loop to flip pixels on O image\n",
    "for i in range():\n",
    "    # Conditionals to flip pixels\n",
    "\n",
    "# Calculate accuracy of model on noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this procedure to at least 5 additional noise levels ranging from 5% to 50% noise (in a loop). Plot the cost of your network as a function of noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input\n",
    "\n",
    "\n",
    "# Define threshold and matrix of random numbers\n",
    "Thresh = np.array([.5, .6, .7, .8, .9])\n",
    "\n",
    "# Initialize data structures for cost\n",
    "randCost = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Define threshold\n",
    "    \n",
    "    # Find indices where random numbers cross threshold\n",
    "    \n",
    "    # Loop to flip pixels of X image\n",
    "    for k in range():\n",
    "        # Conditionals to flip pixels\n",
    "        \n",
    "    # Loop to flip pixels of O image\n",
    "    for j in range():\n",
    "        # Conditionals to flip pixels\n",
    "    \n",
    "    # Calculate and store cost of sigmoid neurons on given data\n",
    "\n",
    "# Plotting\n",
    "noiseLVL = 1 - Thresh\n",
    "plt.figure()\n",
    "plt.plot(noiseLVL,randCost)\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is there a realistic way to account for real-world variability? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
