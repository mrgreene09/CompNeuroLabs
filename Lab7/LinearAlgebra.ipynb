{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Linear Algebra Bootcamp</span>\n",
    "\n",
    "Our work with neural networks and neural decoding will require some facility with linear algebra. Some of you have taken this course, and others have not. Even if you have taken linear algebra, we'll go deeper into the applications of this topic than is normally covered in a pure math class. The goals of this tutorial are:\n",
    "\n",
    "- Bring everyone up to speed on the linear algebra used in artificial neural networks.\n",
    "- Give everyone a visual, conceptual, and applied view of linear algebra.\n",
    "- Show two important applications in applied linear algebra: PCA and linear regression. These are two of the major work horses in modern data analysis.\n",
    "\n",
    "<img src=\"image1.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "### Part 1: Vectors\n",
    "\n",
    "In high school, you were probably told that a vector is something with a magnitude and a direction, such as a car moving east for 40 miles. In this course, we've been treating vectors largely as a list of numbers (voltages over time, for example). On the surface, these seem very different. On a deeper level, these are descriptions of the same thing!\n",
    "\n",
    "Let's start with a simple example. Let's say that the first two voltages in your Hodgkin and Huxley simulation are -65 and -64.5. Create this vector and plot it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create the 2-element vector\n",
    "\n",
    "# Step 2: Plot the vector. Hint: replace x and y with the vector elements\n",
    "plt.figure()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that the third voltage that you measured was -62. Add this to your vector and alter the plotting code to show it in 3d.\n",
    "\n",
    "Note: When matplotlib was originally created, it didn't have the ability to plot in 3-dimensions. However, with the mpl_toolkit, you can pass a parameter into plt.axes called projection='3d' which can fix this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for plotting\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Step 1: Add to the vector\n",
    "\n",
    "# Step 2: Plot in 3-D\n",
    "ax.plot3D()\n",
    "ax.set_xlabel('Voltage at t=1')\n",
    "ax.set_ylabel('Voltage at t=2')\n",
    "ax.set_zlabel('Voltage at t=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are creating here is a voltage *space*. Imagine that we select 1000 triplets of voltages, each with random values between -75 and +40 mV. What will the cloud of vectors look like? In the cell below, write a for-loop which randomly selects 3 integers between -75 and +40 mV and draw them on the same 3-dimensional plot 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import plotting packages to plot in a new figure\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "for i in range(1000):\n",
    "    # Select random voltages\n",
    "    \n",
    "    # Plotting\n",
    "    ax.plot3D()\n",
    "    ax.set_xlabel('Voltage at t=1')\n",
    "    ax.set_ylabel('Voltage at t=2')\n",
    "    ax.set_zlabel('Voltage at t=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that a list of numbers can exist as a trajectory in an abstract space. The length of these vectors is determined by their distance from the origin (0,0). This is know as the *norm* of the vector, and is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\lvert x \\rvert _2 = \\lvert x \\rvert = \\sqrt{{x_1}^2 + {x_2}^2 + ...{x_n}^n}\n",
    "\\end{equation*}\n",
    "\n",
    "Compute the norm of the vector v = [1, 2, 3] by hand using this equation. Then use the *norm* function to verify your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing norm function\n",
    "from numpy.linalg import norm\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who like to think visually, this is just an implementation of the Pythagorean theorem.\n",
    "\n",
    "<img src=\"image2.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "In some applications, we will want our vectors to have a length (or norm) of 1. Create a new vector *v2* that is the unit length vector in the same direction as *v*. Plot both vectors below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Plotting\n",
    "plt.figure(3)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with adding and multiplying a scalar to a vector. Define vector v to be [2, 3]. Create new vectors v2 and v3 that add or multiply 6 to vector *v* respectively. Use the plotting code to visualize the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define v, v2, and v3\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.plot(,c='b')\n",
    "plt.plot(,c='r')\n",
    "plt.plot(, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your graph, can you guess why multiplying a vector by a scalar is called scaling the vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dot Product\n",
    "\n",
    "In our neural networks, we will be using the dot product a lot. It's also worth noting that you have already seen the dot product when we covered convolution in the last few labs. The dot product can be defined in two different ways:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha = \\sum_{i=1}^na_ib_i = a^Tb\n",
    "\\end{equation*}\n",
    "\n",
    "There are four different ways one could code this up in Python. The first way is to use a for-loop to do the summation. Create two row vectors $a$ and $b$ that are each 100 values drawn from a standard normal distribution (hint: use np.random.randn). Use a for loop to loop over each value of a and b, compute their product, and then sum the resulting vector of products outside of the loop. Call the final result *dot1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a and b\n",
    "\n",
    "# Intialize data structure\n",
    "dot = np.zeros()\n",
    "# Loop through a and b to calculate dot product\n",
    "for i in range():\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be doing this operation hundreds of times in our neural networks, we won't want to use loops because they are very slow. The second way is to sum the *elementwise* product of $a$ and $b$. In one line of code, implement this method and assign it to dot2.\n",
    "\n",
    "Hint: Use numpy's multipy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot2 = np.sum(np.multiply())\n",
    "dot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third method is to use the second definition above and multiply the transpose of $a$ with $b$.  In order to do this, use numpy's *transpose* and *matmul* functions. (Hint: If you haven't already realized this, Google, Stack Overflow, and the SciPy.org are your friends!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot3 = np.matmul(,)\n",
    "dot3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method is to use the numpy function *dot*. Assign *dot3* to method three, and assign *dot4* to method four. If you have implemented these four methods correctly, they should all yield the same results.\n",
    "\n",
    "Hint: numpy's *dot* function only works with 1-D arrays, index the $a$ and $b$ vectors accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot4 = np.dot()\n",
    "dot4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric interpretation of the dot product is also important. The dot product of $a$ and $b$ can tell us how much of a vector $a$ points in the same direction as vector $b$. Sometimes this is stated as the lenght of $a$ when projected onto $b$. I like to think of a flashlight shining down on vector $a$ onto vector $b$. The dot product would reflect the length of the shadow.\n",
    "\n",
    "<img src=\"image3.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "This leads us to our third definition of the dot product:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha = |a|  |b|cos(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "It should be clear to see that when your two vectors are unit vectors, then their dot product is simply the cosine of the angle between them. This turns out to have an important link to the idea of correlation.\n",
    "\n",
    "Consider the following vectors below. Play around with plotting pairs of them so that you understand their spatial relations. Turn them into unit vectors and compute their dot products. What do you conclude about the dot product when the angle between the vectors is less than 90 degrees? Is more than 90 degrees? Is equal to 90 degrees? In what way does this remind you of correlation? How does this correspond to the picture that you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([0, 1])\n",
    "v3 = np.array([-.5, 1])\n",
    "\n",
    "# Turn vectors into unit length\n",
    "v1n = \n",
    "v2n = \n",
    "v3n = \n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, v1n[0]], [0, v1n[1]],'b')\n",
    "plt.plot([0, v2n[0]], [0, v2n[1]],'r')\n",
    "plt.plot([0, v3n[0]], [0, v3n[1]],'k')\n",
    "\n",
    "# Explore the dot products between them to map onto what you see in the graph\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does scalar multiplication have an impact on the dot product between two vectors? Explore some examples to find a pattern. In a comment, write 3-5 sentences to provide a geometric intuition for what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([2, 4, 9])\n",
    "w = np.array([6, 8, 5])\n",
    "dot1 = np.dot(v,w)\n",
    "\n",
    "v2 = v*2\n",
    "w2 = w*2\n",
    "dot2 = np.dot(v2,w2)\n",
    "\n",
    "v3 = v*3\n",
    "w3 = w*3\n",
    "dot3 = np.dot(v3,w3)\n",
    "\n",
    "v4 = v*4\n",
    "w4 = w*4\n",
    "dot4 = np.dot(v4,w4)\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([dot1, dot2, dot3, dot4])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y)\n",
    "\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product (a.k.a inner product) should not be confused with the outer product.\n",
    "\n",
    "- The **np.matmul**(_array1, array2_) function returns the inner product while the **np.outer**(_array1, array2_) function returns the outer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(100)\n",
    "b = np.random.rand(100)\n",
    "\n",
    "inner = np.matmul(a.T, b)\n",
    "outer = np.outer(a, b)\n",
    "\n",
    "print(inner.shape)\n",
    "print(outer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the dimensionality of the inner and outer products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Matrices\n",
    "\n",
    "Matrix multiplication follows different rules from that of normal scalar math. In particular, not all matrices can be multiplied together. Here is the visual rule for matrix multiplication:\n",
    "\n",
    "<img src=\"image5.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "In general, the inner dimensions of the two matrices have to be the same. The resulting matrix will have the same dimensionality as the outer dimensions.\n",
    "\n",
    "The algorithm for matrix multiplication is easy to look up, but I won't be going into it here. Rather, I want you to conceptually understand the result as an ordered series of dot products between the *rows* of matrix A and the *columns* of matrix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice with rules of matrix multiplication\n",
    "m = 4\n",
    "n = 3\n",
    "k = 6\n",
    "\n",
    "# make random matrices\n",
    "A = np.random.rand(m,n)\n",
    "B = np.random.rand(n,k)\n",
    "C = np.random.rand(m,k)\n",
    "\n",
    "# Test which multiplications are valid. Think of your answer first, than test it (hint: comment out other operations)\n",
    "np.matmul(A,B) # Answer:\n",
    "np.matmul(A,A) # Answer:\n",
    "np.matmul(B,C) # Answer:\n",
    "np.matmul(C,B) # Answer:\n",
    "np.matmul(A.T,C) # Answer:\n",
    "np.matmul(B.T,B) # Answer:\n",
    "np.matmul(C.T,B) # Answer:\n",
    "np.matmul(C,B.T) # Answer:\n",
    "np.matmul(B,B.T) # Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipying a matrix and a vector\n",
    "\n",
    "There's a specific way that I would like for you to think about matrix-vector multiplication: I want you to think of a matrix as applying a linear operation to a vector. This could scale the vector, shift it, flip it on its axis, etc.\n",
    "\n",
    "Fill in the following code to get a sense of how vector v becomes vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2-D input vector\n",
    "v = np.array([3, -2])\n",
    "\n",
    "# Create 2x2 transformation matrix\n",
    "A = np.array([[1, -1],[2, 1]])\n",
    "\n",
    "# Create a vector w that is the product of the matrix A multiplied by v\n",
    "# Hint: Remember dimensionality as well as matmul.\n",
    "w = np.matmul()\n",
    "\n",
    "# Plot vector v in black and vector w in red as before\n",
    "plt.figure()\n",
    "plt.plot(,'k')\n",
    "plt.plot(,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the columns of the transformation matrix as providing new coordinants for the axes of the graph. These are known as basis vectors. Choose an angle to complete the code below that creates a pure transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2-D input vector\n",
    "v = np.array([3,2])\n",
    "\n",
    "# Rotation angle (specify in radians)\n",
    "myAngle = \n",
    "theta = np.pi/myAngle\n",
    "\n",
    "# 2x2 Transformation matrix\n",
    "A = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "# Output vector is Av'. Hint: Remember dimensionality\n",
    "w = np.matmul()\n",
    "\n",
    "# Plot them\n",
    "plt.figure()\n",
    "plt.plot(,'k')\n",
    "plt.plot(,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Rank:\n",
    "\n",
    "The rank of a matrix is determined by the number of linearly independent columns it has. A column is linearly independent it could not be formed by a combination of any of the other columns. \n",
    "\n",
    "Rank can range from 1 (all columns are linear combinations) to the total number of columns in the matrix (all columns are linearly independent). You can think of the rank as the number of independent dimensions of data a matrix has. This case is known as a full rank. When a matrix is not full rank, we have a number of judgy-sounding phrases for it: rank deficient, degenerate, low-rank, or singular.\n",
    "\n",
    "Use the **numpy.random.rand()** function to make a 4x4 matrix of random numbers. This random matrix is very unlikely to have linearly dependent columns. Check the rank of the matrix using numpy's *ndim* function. Next, change the second column of the matrix to be a copy of the first, and see what happens to the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random integers\n",
    "A = np.random.rand()\n",
    "rank1 = np.linalg.matrix_rank()\n",
    "\n",
    "# Create a rank deficient matrix\n",
    "\n",
    "rank2 = np.linalg.matrix_rank()\n",
    "\n",
    "print(rank1)\n",
    "print(rank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank deficiencies can lead to all types of troubles in linear regression because it means that your model is more complex that the amount of information you have. Bad times. There are a number of ways to fix of regularize your matrix to get it back to full rank. One method is to add a tiny bit of random noise. Create a 10x10 rank-deficient matrix, compute its rank, then add 0.001*np.random.randint() to the size of your matrix and re-compute its rank. In a comment, note whether this is a justifiable choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inverse\n",
    "\n",
    "You can think of the inverse of a matrix like the equivilent of matrix division. Consider the following equation from scalar algebra:\n",
    "\n",
    "\\begin{equation*}\n",
    "3x = 1\n",
    "\\end{equation*}\n",
    "\n",
    "Your first instinct for oslving this equation is to divide by 3. An equivalent expression is:\n",
    "\n",
    "\\begin{equation*}\n",
    "3^{-1}3x=3^{-1}1\n",
    "\\end{equation*}\n",
    "\n",
    "You can easily see here that the inverse of 3 is 1/3.\n",
    "\n",
    "In a similar manner, the inverse of a matrix is the matrix that, when multiplied by the original matrix, gives the identity matrix (the matrix equivalent to 1). In other words:\n",
    "\n",
    "\\begin{equation*}\n",
    "AA^{-1}=A^{-1}A=I\n",
    "\\end{equation*}\n",
    "\n",
    "Critically, not all matrices are invertible. Only square and full-rank matrices can be inverted.\n",
    "\n",
    "Create a 3x3 matrix A with random numbers using *randint*. Then, use *inv* to invert this matrix. Then, multipy the two matrices together to get the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear algebra library from numpy\n",
    "from numpy.linalg import inv\n",
    "# Create matrix of random integers\n",
    "\n",
    "# Use the inv function to get the inverse of A\n",
    "Ainv = inv(A)\n",
    "\n",
    "# Remember matmul\n",
    "np.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:**\n",
    "The *inv* function is not fully accurate and your answers will get progressively worse as the matrices get larger. I recommend using numpy and scipy packages to do regression rather that the methods shown below. These are just for learning.\n",
    "\n",
    "### Part 3: Ordinary Least Squares Regression\n",
    "\n",
    "The goal of least square regression is to predict the value of some dependent variable *y* based on a set of independent observations that are placed into matrix X. $\\beta$ refers to the vector weights that are associated with each observation.\n",
    "\n",
    "\\begin{equation*}\n",
    "X\\beta=y\n",
    "\\end{equation*}\n",
    "\n",
    "For example, let's say that we are trying to predict the price of an apartment based on the number of bedrooms, its proximinity to public transportaiton, and its age. Let's say that we have a dataset of 100 apartments with each of these three measurements. X is therefore a 100x3 matrix, and the vector y is the monthly rent for the apartment. $\\beta$ then tells us how much each of the three variables contributes to the cost of rent, while holding the others constant. Using this model, we can then make an educated guess about the cost of a new apartment, based only on those three variables.\n",
    "\n",
    "In order to solve this equation, we would like to invert X, but X is not square. Fortunately, we can use the left-inverse ${(X^TX)}^{-1}X^T$ as follows:\n",
    "\n",
    "$${(X^TX)}^{-1}X^TX\\beta = {(X^TX)}^{-1}X^Ty$$\n",
    "$$\\beta = {(X^TX)}^{-1}X^Ty$$\n",
    "\n",
    "\n",
    "Note: For this to work, matrix X must be full-rank. Imagine that you had included area in meters squared as well as feet. These would be linearly dependent and would not give us a correct $\\beta$.\n",
    "\n",
    "Most of the time, our solution for $\\beta$ will not be exact. Therefore, it's best to denote the equation as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}=X\\hat{\\beta}+\\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "Where the \"hats\" refer to our best estimations of $y$ and $\\beta$. The $\\epsilon$ refers to the errors (or residuals) of the model.\n",
    "\n",
    "Using scipy.io's *loadmat* function, load the trafficAccidents.mat file into your notebook. This is aggregate data from all 50 states plus Washington DC on fatal traffic accidents. The columns of the data matrix are:\n",
    "1. Number of licensed drivers\n",
    "2. Miles traveled per vehicle\n",
    "3. Percent of alcohol related accidents\n",
    "4. Total population\n",
    "5. Percent urban population\n",
    "\n",
    "The y vector refers to the total number of traffic fatalities.\n",
    "\n",
    "Add a first column to the data matrix that is a column of ones. This will help us more reliably estimate an intercept for our model. Then, estimate B using the left division alternative to the inverse that you learned before. Last, compute the predicted *y* values, $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlaoding data\n",
    "import scipy.io as sio\n",
    "trafficAccidents = sio.loadmat('trafficAccidents.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data and y variables\n",
    "data = trafficAccidents['data']\n",
    "y = trafficAccidents['y']\n",
    "\n",
    "# Create matrices\n",
    "A = np.ones()\n",
    "data = np.concatenate((A,data), axis=1)\n",
    "\n",
    "XtX = inv(np.matmul())\n",
    "XtY = np.matmul()\n",
    "B = np.matmul()\n",
    "    \n",
    "yHat = np.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is your model? Using **_np.corrcoef_** to find the correlation coefficient between the y and yHat vectors. Square this value and the result is the proportion of variability in y that you can predict with your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at your B matrix and determine which variable contributes most to highway fatalities.\n",
    "\n",
    "Note: Remeber that the first column corresponds to the column of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a surprising turn of events, Puerto Rico has now been named as the 51st U.S. state. Load the *y_PR.npy* and *data_PR.npy* with the **_np.load()_** function and add them to the pre-existing data. Then, using the same method of calculating yHat as you learned earlier, find the prediction of how many highway fatalities there will be in Puerto Rico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data for Puerto Rico\n",
    "y_PR = np.load('y_PR.npy')\n",
    "data_PR = np.load('data_PR.npy')\n",
    "\n",
    "# Concatenating data\n",
    "newData = np.vstack((data,data_PR))\n",
    "newY = np.append(y,y_PR)\n",
    "\n",
    "#Calculating yHat\n",
    "XtX = inv(np.matmul())\n",
    "XtY = np.matmul()\n",
    "B = np.matmul()\n",
    "yHat = np.matmul()\n",
    "\n",
    "# Find the prediction for Puerto Rico\n",
    "yHat[51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the rest of the country? Answer in a comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work today! Don't forget to save this in your etna folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
