{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Firing Rate and Spike Train Statistics</span>\n",
    "\n",
    "As you recall from class, there are several ways to think about a neuron's firing rate. In the first part of this lab, we will work with two types of firing rates, as well as calculate the Fano Factor and the inter-spike interval (ISI) distribution of the neuron.\n",
    "\n",
    "Note: Because we will be using data from a single neuron, we will omit the population method.\n",
    "\n",
    "<img src=\"image1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The data we will be using was collected from de Ruyter van Stevenick from a blow fly (Calliphora erytrocephela) H1 visual neuron responding to a white noise visual motion stimulus. They recorded the neuron at 500 Hz (in other words, each sample represents 2 ms). Within the file, $rho$ is a boolean vector where 1 denotes a spike and $stim$ is a variable indicating the intensity of the visual stimulus (in this case, the angular velocity of the random moving dots). We will only be using the rho vector today, but we will return to the full data set next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages to load data: run this to get your data\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Loading data\n",
    "with open('rho.csv') as f:\n",
    "    # storing data to a variable called rho\n",
    "    rho = list(csv.reader(f))\n",
    "\n",
    "# Converting the file to an array of floats\n",
    "rho = np.array(rho[:], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward method for computing a neuron's firing rate is the **spike count rate**. Here, we simply count the number of spikes and divide by the interval of time:\n",
    "\n",
    "\\begin{equation*}\n",
    "r = \\frac{n_{spikes}}{time}\n",
    "\\end{equation*}\n",
    "\n",
    "Compute the spike count rate in spikes per second (Hz) for the entire duration of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can characterize this neuron by its Fano factor. Recall that this is the variance of the spike count rate over its mean:\n",
    "\n",
    "\\begin{equation*}\n",
    "F = \\frac{{\\sigma_{sp}}^2}{\\mu_{sp}}\n",
    "\\end{equation*}\n",
    "\n",
    "For neurons that are well-characterized by a Poisson process, their Fano factor should equal 1.\n",
    "\n",
    "In order to get the mean and variability, break the recording up into non-overlapping one-second intervals, and calculate the spike count rate in each interval. From this data, compute the mean, variance, and Fano factor.\n",
    "\n",
    "Hint: np.mean() and np.var() will help you out here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented properly, you should come up with a Fano factor of $\\approx$ 6.23.\n",
    "\n",
    "Another way to characterize the neuron's firing pattern is to examine the distribution of interspike intervals (ISI). Unlike the Hodgkin and Huxley model that fired spikes at a constant pace, real neurons have some variability. Compute the ISI between each consecutive pair of spikes, and plot this ISI distribution as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Find the indices of spikes in rho. Hint: np.where() or np.nonzero() will be helpful!\n",
    "\n",
    "# Initialize data structure\n",
    "isi = np.zeros()\n",
    "\n",
    "# Initialize counter for indexing\n",
    "x = 0\n",
    "# For-loop to calculate the ISI\n",
    "for i in range():\n",
    "\n",
    "\n",
    "# plot isi in a histogram\n",
    "plt.figure()\n",
    "plt.hist(isi,100);\n",
    "plt.xlabel(#FILL ME IN)\n",
    "plt.ylabel(#FILL ME IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your ISI and Fano results, does this neuron appear to be well-modeled by a Poisson process? Justify your answer as a comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare this to an actual Poisson process. In this exercise, we'll be creating our own artificial spike train. Recall that for a Poisson process, the probability of a spike occuring in some small time interval $t$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\lambda^ke^-\\lambda}{k!}\n",
    "\\end{equation*}\n",
    "\n",
    "Create a Poisson spike generator that simulates a neuron that spikes at a constant rate of $45 Hz$. Your simulator should sample $1000 Hz$. Run your simulator for 1000 runs of 1 second each, and record the spikes in a single matrix. Compute the following:\n",
    "- A histogram of spike count rate\n",
    "- A histogram of interspike intervals\n",
    "- Fano factor for spike counts obtained over the 1000 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability of a spike occuring\n",
    "prob = \n",
    "\n",
    "# Initiate data structure to hold the spikes\n",
    "spikeMatrix = np.zeros()\n",
    "\n",
    "# Create a nested loop to simulate 1000 trials of 1000 samples: \n",
    "for i in range():\n",
    "    for j in range():\n",
    "        # Conditional to assign a spike to spikeMatrix\n",
    "        if np.random.rand() < :\n",
    "            # Store a 1 in spikeMatrix\n",
    "            \n",
    "# Calculate number of spikes (hint: we are summing across trials)\n",
    "spikeCountRates = \n",
    "\n",
    "# Set up data stuctures: (hint: find total number of spikes first)\n",
    "isi = np.zeros()\n",
    "\n",
    "# Create a loop that finds the spikes in spikeMatrix \n",
    "for i in range():\n",
    "    # Find indices of spikes\n",
    "    \n",
    "    \n",
    "    for j in range(len(spikes)-1):\n",
    "        # Calculate ISI and store in vector\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(spikeCountRates,40,edgecolor='black');\n",
    "plt.xlabel(\"Average firing rate (Hz)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(isi,100);\n",
    "plt.xlabel(\"ISI (ms)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Calculate Fano factor\n",
    "Fano2 = \n",
    "print(Fano2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other applications, we want to be able to express the neuron's firing rate as a more continuous value. If we had multiple trials, we could create  **peri-stimulus-time histogram** as described in your reading. However, the blowfly dataset reflects a continuous recording with only one \"trial\". However, we can move from discrete to continuous by examining the spike-count rate in smaller and smaller windows\n",
    "\n",
    "Use a 5-point sliding window to approximate a continuous firing rate function. Your first point will reflect the proportion of time points containing spikes of the first five points of $rho$. Your second point will reflect the spike proportion of points 2-6, etc. Pay attention to the end of the recording: your loop will truncate five time points before the end. Plot the first 1000 points of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data structure\n",
    "smoothRate = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Calculate proportion of points with spikes in each window and store in smoothRate\n",
    "    \n",
    "# Plotting\n",
    "plt.plot(smoothRate[1:1000])\n",
    "plt.xlabel(\"Time Points\")\n",
    "plt.ylabel(\"Smoothed firing rate (Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the code that you just created is that it has a time lag: your first datapoint in your smoothed function reflects the neuron's activity up to 10 ms in the future! To reduce the time delay, we can express each smoothed point as the *center* of five points in $rho$.\n",
    "\n",
    "Hint: You will need to truncate both the beginning and end of the recording window.  \n",
    "Hint: I recommend using 2 points before each time point and 3 points after. This is technically a six-point window, but it will actually show you that these are the same except for how Python handles the \"overflow\" at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data structure\n",
    "smoothRate2 = np.zeros()\n",
    "\n",
    "for i in range():\n",
    "    # Calculate firing rate in each window and store in smoothRate2\n",
    "\n",
    "# Plotting\n",
    "plt.plot(smoothRate2[1:1000])\n",
    "plt.xlabel(\"Time Points\")\n",
    "plt.ylabel(\"Smoothed firing rate (Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following line and compare this result with the one that you just wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothRate3 = np.convolve(rho[1:1000,0], [.2, .2, .2, .2, .2],'same')\n",
    "\n",
    "# Plotting\n",
    "plt.plot(smoothRate3[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations - you just ran your first convolution by hand! Although the word sounds intimidating, you can think of a convolution as a weighted average between one vector called *signal*, and another called the *kernel*. (Not related to iPython kernels)\n",
    "\n",
    "<img src=\"image2.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "In the previous example, the kernel is [.2, .2, .2, .2, .2] indicating that each of the five points contributes equally to the resulting vector. One can change these weights to decrease the influence of data points that are further in the past or future.\n",
    "\n",
    "\n",
    "In your 5-point moving window, you will see that there are only a few different firing rates represented. Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your original moving-window method, create two new smoothed firing rate vectors, one using 11-point moving window, and the other using a 51-point moving window. Examine the histograms of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# 11 point window\n",
    "# Initialize data structure\n",
    "smoothRate4 = np.zeros()\n",
    "for i in range():\n",
    "    # Calculate firing rate in each window and store in smoothRate4\n",
    "\n",
    "# 51 point window\n",
    "# Initialize data structure\n",
    "smoothRate5 = np.zeros()\n",
    "for i in range():\n",
    "    # Calculate firing rate in each window and store in smoothRate5\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(smoothRate4[1:1000])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(smoothRate5[1:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a comment, discuss the relative advantages and disadvantages of each in 2-3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we might want to do is the measure the degree of randomness in a spike train (whether it is a real spike train like *rho* or a simulated one like you Poisson model). One way of doing this is with an <u>autocorrelation function</u>. An autocorrelation examines the correlation between a time series and a time-shifted version of itself from both the past and the future. It tells us how similar the data are to itself over time (a random process will not have any self-similarity except when the time shift is 0). To understand autocorrelation, we must first recall the definition of correlation:\n",
    "\n",
    "\\begin{equation*}\n",
    "r = \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2}\\sqrt{\\sum(y_i-\\bar{y})^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where the summations are over observations, and $\\overline{x}$ and $\\overline{y}$ are the mean of $x$ and $y$ respectively.\n",
    "\n",
    "Whereas correlation measures the association between two variables $x$ and $y$, the autocorrelation measures the association between variable x and a time-shifted version of itself. For example, in a first-order autocorrelation, the time lag is 1:\n",
    "\n",
    "\\begin{equation*}\n",
    "r_1 = \\frac{\\sum_{i=1}^{N-1}(x_i-\\bar{x})(x_{i+1}-\\bar{x})}{\\sum_{i=1}^{N}(x_i-\\bar{x})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "For any vector length m, the autocorrelation function will be length $2*m+1$ and will cover all possible lags.\n",
    "\n",
    "Run the **plt.xcorr(**_array1, array2_**)** code below to compute the autocorrelation of the first 1000 time points of the rho vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making rho 1-Dimensional\n",
    "rho1 = rho[:,0]\n",
    "\n",
    "plt.xcorr(rho1[:1000], rho1[:1000], maxlags=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the resulting figure mean? How might we intepret the peak (what its value represents and where it is located in the function)? Why are there lower values on either side of the peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use **plt.xcorr(**_array1, array2_**)** on a Poisson spike train and compare the results to what you saw in the previous autocorrelation function. In a comment, tell me what evidence you see from the graph that the Poisson spike train is more random than the H1 spike train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work today! As always, please upload this file to Lyceum for grading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
